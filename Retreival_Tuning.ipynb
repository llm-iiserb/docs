{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b715375",
   "metadata": {},
   "source": [
    "# Retreival Tuning\n",
    "\n",
    "In this tutorial we will see a simple example of basic retrieval via Top-K Similarity search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d615a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain --upgrade\n",
    "# Version: 0.0.164\n",
    "\n",
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d3e92ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDF Loaders. If unstructured gives you a hard time, try PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, TextLoader, PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166d759",
   "metadata": {},
   "source": [
    "### Load your data\n",
    "\n",
    "Next let's load up some data. I've put a few 'loaders' on there which will load data from different locations. Feel free to use the one that suits you. The default one queries one of Paul Graham's essays for a simple example. This process will only stage the loader, not actually load it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a2d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader = TextLoader(file_path=\"../data/PaulGrahamEssays/vb.txt\")\n",
    "\n",
    "## Other options for loaders \n",
    "#loader = PyPDFLoader(\"pdf_main/student_manual_23.pdf\")\n",
    "dir_loader = PyPDFDirectoryLoader(\"docs_student/\")\n",
    "# loader = UnstructuredPDFLoader(\"../data/field-guide-to-data-science.pdf\")\n",
    "# loader = OnlinePDFLoader(\"https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d38044",
   "metadata": {},
   "source": [
    "Then let's go ahead and actually load the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcdac23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dir_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a744a",
   "metadata": {},
   "source": [
    "Then let's actually check out what's been loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4fd7c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 373 document(s) in your data\n",
      "There are 0 characters in your sample document\n",
      "Here is a sample: \n"
     ]
    }
   ],
   "source": [
    "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your sample document')\n",
    "print (f'Here is a sample: {data[0].page_content[:200]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af9b604",
   "metadata": {},
   "source": [
    "### Chunk your data up into smaller documents\n",
    "\n",
    "While we could pass the entire essay to a model w/ long context, we want to be picky about which information we share with our model. The better signal to noise ratio we have the more likely we are to get the right answer.\n",
    "\n",
    "The first thing we'll do is chunk up our document into smaller pieces. The goal will be to take only a few of those smaller pieces and pass them to the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a527b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat all the data into one string\n",
    "all_text = \"\"\n",
    "for doc in data:\n",
    "    all_text += doc.page_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91305c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb3c6f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaunthesheep/anaconda3/envs/llmsearch-u/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      ".gitattributes: 100%|██████████| 1.52k/1.52k [00:00<00:00, 2.85MB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 191/191 [00:00<00:00, 311kB/s]\n",
      "README.md: 100%|██████████| 90.3k/90.3k [00:00<00:00, 475kB/s]\n",
      "config.json: 100%|██████████| 779/779 [00:00<00:00, 5.54MB/s]\n",
      "config_sentence_transformers.json: 100%|██████████| 124/124 [00:00<00:00, 251kB/s]\n",
      "model.safetensors: 100%|██████████| 1.34G/1.34G [00:12<00:00, 106MB/s] \n",
      "pytorch_model.bin: 100%|██████████| 1.34G/1.34G [00:12<00:00, 110MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 52.0/52.0 [00:00<00:00, 105kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 262kB/s]\n",
      "tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 1.27MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 366/366 [00:00<00:00, 747kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.26MB/s]\n",
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 703kB/s]\n"
     ]
    }
   ],
   "source": [
    "# We'll split our data into chunks around 500 characters each with a 50 character overlap. These are relatively small.\n",
    "\n",
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "text_splitter = SemanticChunker(HuggingFaceEmbeddings(model_name = \"BAAI/bge-large-en-v1.5\"), threshold=95, max_chunk_size=2048)\n",
    "\n",
    "#texts = text_splitter.split_documents(data)\n",
    "texts = text_splitter.create_documents([all_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "879873a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 475 documents\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many small chunks we have\n",
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f51230bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([131.,  26.,  17.,  17.,   8.,  15.,  15.,  11.,   9., 226.]),\n",
       " array([2.0000e+00, 2.0660e+02, 4.1120e+02, 6.1580e+02, 8.2040e+02,\n",
       "        1.0250e+03, 1.2296e+03, 1.4342e+03, 1.6388e+03, 1.8434e+03,\n",
       "        2.0480e+03]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist([len(text.page_content) for text in texts], bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b2843",
   "metadata": {},
   "source": [
    "### Create embeddings of your documents to get ready for semantic search\n",
    "\n",
    "Next up we need to prepare for similarity searches. The way we do this is through embedding our documents (getting a vector per document).\n",
    "\n",
    "This will help us compare documents later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e7857",
   "metadata": {},
   "source": [
    "Check to see if there is an environment variable with you API keys, if not, use what you put below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3205993a",
   "metadata": {},
   "source": [
    "Then we'll get our embeddings engine going. You can use whatever embeddings engine you would like. We'll use OpenAI's ada today.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75510077",
   "metadata": {},
   "outputs": [],
   "source": [
    "del text_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d66c06",
   "metadata": {},
   "source": [
    "### Option #1: Chroma (for local)\n",
    "\n",
    "I like Chroma becauase it's local and easy to set up without an account.\n",
    "\n",
    "First we'll pass our texts to Chroma via `.from_documents`, this will 1) embed the documents and get a vector, then 2) add them to the vectorstore for retrieval later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "373e695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4619d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaunthesheep/anaconda3/envs/scrape/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/shaunthesheep/anaconda3/envs/scrape/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-12.1/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /home/shaunthesheep/anaconda3/envs/scrape/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaunthesheep/anaconda3/envs/scrape/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/shaunthesheep/anaconda3/envs/scrape did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/shaunthesheep/anaconda3/envs/scrape/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name = \"maidalun1020/bce-embedding-base_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0d1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into Chroma\n",
    "#vectorstore = Chroma.from_documents(texts, embeddings, persist_directory=\"./chromadb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6078dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chroma embeddings from persistingdirectory\n",
    "vectorstore = Chroma(persist_directory=\"./chromadb\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35dcd9",
   "metadata": {},
   "source": [
    "### Query those docs to get your answer back\n",
    "\n",
    "Great, those are just the docs which should hold our answer. Now we can pass those to a LangChain chain to query the LLM.\n",
    "\n",
    "We could do this manually, but a chain is a convenient helper for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799af1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bfec80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f051337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ac6f238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a helpful assistant for students that generates multiple search queries based on a source question.\\n    Generate 3 other queries which contain just the semantic keywords related to: ```How to apply for hostel leave``` \\n\\n    Generated queries should be separated by newlines, no other text should be there.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiquery_prompt = \"\"\"You are a helpful assistant for students that generates multiple search queries based on a source question.\n",
    "    Generate {n_versions} other queries which contain just the semantic keywords related to: ```{question}``` \n",
    "\n",
    "    Generated queries should be separated by newlines, no other text should be there.\"\"\"\n",
    "multiquery_prompt = PromptTemplate.from_template(multiquery_prompt)\n",
    "n_versions = 3\n",
    "question = \"How to apply for hostel leave\"\n",
    "multiquery_prompt.format(n_versions=n_versions, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0379828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b9b1c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaunthesheep/anaconda3/envs/scrape/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1085: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/shaunthesheep/anaconda3/envs/scrape/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "model.safetensors.index.json: 100%|██████████| 25.1k/25.1k [00:00<00:00, 17.0MB/s]\n",
      "model-00001-of-00003.safetensors: 100%|██████████| 4.94G/4.94G [00:45<00:00, 109MB/s]\n",
      "model-00002-of-00003.safetensors: 100%|██████████| 5.00G/5.00G [00:45<00:00, 110MB/s]\n",
      "model-00003-of-00003.safetensors: 100%|██████████| 4.54G/4.54G [00:42<00:00, 107MB/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [02:14<00:00, 44.75s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.03it/s]\n",
      "generation_config.json: 100%|██████████| 111/111 [00:00<00:00, 220kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, you need an access token\n",
    "hf_auth = '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'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a81a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd4a23a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vllm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vllm'"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30331d17",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m retriever_from_llm \u001b[38;5;241m=\u001b[39m \u001b[43mMultiQueryRetriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiquery_prompt\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/scrape/lib/python3.10/site-packages/langchain/retrievers/multi_query.py:90\u001b[0m, in \u001b[0;36mMultiQueryRetriever.from_llm\u001b[0;34m(cls, retriever, llm, prompt, parser_key, include_original)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize from llm using default template.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    MultiQueryRetriever\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m output_parser \u001b[38;5;241m=\u001b[39m LineListOutputParser()\n\u001b[0;32m---> 90\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m     92\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mretriever,\n\u001b[1;32m     93\u001b[0m     llm_chain\u001b[38;5;241m=\u001b[39mllm_chain,\n\u001b[1;32m     94\u001b[0m     parser_key\u001b[38;5;241m=\u001b[39mparser_key,\n\u001b[1;32m     95\u001b[0m     include_original\u001b[38;5;241m=\u001b[39minclude_original,\n\u001b[1;32m     96\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/scrape/lib/python3.10/site-packages/langchain_core/load/serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/scrape/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)"
     ]
    }
   ],
   "source": [
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever, llm=model, prompt=multiquery_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae638cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a response basemodel using pydantic\n",
    "from pydantic import create_model\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6c892ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"### Instruction:\n",
    "      You are a helpful AI search assistant for IISER Bhopal(a university) students. Use the following pieces of context to answer the question at the end. If answer isn't in the context, say that you don't know, don't try to make up an answer. Answer in bullet points.\n",
    "      ### Context:\n",
    "      ---------------\n",
    "      {context}\n",
    "      ---------------\n",
    "      \n",
    "      ### Question: {question}\n",
    "      ### Response:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3aa67e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fcb251c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'langchain.prompts.prompt.PromptTemplate'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m rag_chain_from_docs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mRunnablePassthrough\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmy_prompt\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m|\u001b[39m llm\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m rag_chain_with_source \u001b[38;5;241m=\u001b[39m RunnableParallel(\n\u001b[1;32m      9\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: retriever_from_llm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: RunnablePassthrough()}\n\u001b[1;32m     10\u001b[0m )\u001b[38;5;241m.\u001b[39massign(answer\u001b[38;5;241m=\u001b[39mrag_chain_from_docs)\n\u001b[1;32m     12\u001b[0m response \u001b[38;5;241m=\u001b[39m rag_chain_with_source\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow to take hostel leave?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmsearch-u/lib/python3.10/site-packages/langchain_core/runnables/base.py:424\u001b[0m, in \u001b[0;36mRunnable.__or__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__or__\u001b[39m(\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    416\u001b[0m     other: Union[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m     ],\n\u001b[1;32m    422\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RunnableSerializable[Input, Other]:\n\u001b[1;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compose this runnable with another object to create a RunnableSequence.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableSequence(\u001b[38;5;28mself\u001b[39m, \u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmsearch-u/lib/python3.10/site-packages/langchain_core/runnables/base.py:4193\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing)\u001b[0m\n\u001b[1;32m   4191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001b[1;32m   4192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   4194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a Runnable, callable or dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4195\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4196\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'langchain.prompts.prompt.PromptTemplate'>"
     ]
    }
   ],
   "source": [
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: x))\n",
    "    | my_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever_from_llm, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "\n",
    "response = rag_chain_with_source.invoke(\"How to take hostel leave?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d065ce39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      - Fill out the hostel leave form.\n",
      "      - Get it approved by the warden.\n",
      "      - Hand it over to the caretaker before leaving.\n",
      "      - In addition, fill out and get approved the academic leave form issued by the DOAA office. Leave will not be approved by wardens unless the academic leave form is first approved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b20c9e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Page: {} Source: .\n",
      "**********\n",
      "**********\n",
      "Page: {} Source:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
      "**********\n",
      "**********\n",
      "Page: {} Source: le applying for On line hostel leave ) (Web link \n",
      "for hostel leave Click to  Apply . It \n",
      "is “Mandatory  to submit On -line hostel Leave”. Students \n",
      "are required  to produce approved hostel leave at Main \n",
      "Gate Security. Additionally,  before leaving the hostel, \n",
      "write your name, Roll No, date and destination place with \n",
      "Signature in the Hostel Leave Register kept at the Hostel \n",
      "Security. Students are required to carry their Institute ID [11]  \n",
      " card at all times and produce it on demand by the Institute \n",
      "Authorities . Students must return to the hostel by 01.00 \n",
      "a.m. (2.00 am in case of extended library time during \n",
      "exams) and students are also advised not to leave the \n",
      "hostel before 5:00 a.m. These timings may be changed if \n",
      "found necessary for maintaining discipline. Occasionally, \n",
      "if the hostel resident expects to be la te beyond the \n",
      "stipulated time, He/ she  is required to take hostel leave \n",
      "from warden in charge and produce the same before \n",
      "leaving at Institute main gate (as per existing hostel leave \n",
      "ru\n",
      "**********\n",
      "**********\n",
      "Page: {} Source: s per hostel rule no 03 student is  required to apply \n",
      "for On -Line hostel leave to take approv al from warden in \n",
      "charge  and produce the same before leaving at the Institute \n",
      "Main Gate. During a running academic semester it is \n",
      "mandatory to take academic leave and it is also mandatory \n",
      "to upload the approved academic leave while submitting \n",
      "online hostel  leave. During emergency situations (like \n",
      "Medical Emergency Situations in family when a student \n",
      "has to leave the campus on urgent basis)  and when student \n",
      "is unable to take academic leave, the student can write an \n",
      "email to DoAA for Academic leave (clearly e xplaining \n",
      "medical emergency situation along with proof)  and attach \n",
      "the  same email while submitting On line  hostel leave. Upon resuming campus after leave and not submitting \n",
      "hostel leave may be considered an act of indiscipline and [21]  \n",
      " the student may be pena lized with a fine of Rs.1000/ - as \n",
      "per existing penalty/fine for not submitting hostel leave as \n",
      "per hostel rule no 33 (c). 35. Insti\n",
      "**********\n",
      "**********\n",
      "Page: {} Source: ach with the On -Line Hostel Leav e \n",
      "application. 8.1 Vacating hostel during Semester drop/Semester leave  \n",
      " \n",
      "If the student drop s a semester due to medical reason \n",
      "or otherwise approved by DoAA. He/She should be \n",
      "required to vacate  the Hostel Accommodation. 9. CODE OF CONDUCT  \n",
      " \n",
      "Each student should conduct himself/herself in such a \n",
      "manner as appropriate to a student of a reputed \n",
      "Institute. Further, he/she should not indulge in such an \n",
      "act that may bring disrepute to the Institute. He/she is [8]  \n",
      " expected to show courtesy and respect to  teachers, \n",
      "administrators, officers and employees of the Institute. He/she is expected to be courteous to visitors of the \n",
      "Institute, residents of the campus and maintain good \n",
      "neighbourly relations with fellow students. A student who behaves with lack of c ourtesy and \n",
      "decorum, indulges in unbecoming activities (both \n",
      "inside and outside the Institute), causes wilful damage \n",
      "and/or removal of Institute property or belongings of \n",
      "fellow students, disturbs others in the\n",
      "**********\n",
      "**********\n",
      "Page: {} Source: \n",
      "academic office. 27. During the vacation period all undergraduate hostel \n",
      "rooms  MUST  be vacated unless an authorization has been \n",
      "obtained from the warden. Hostel rooms for any academic [18]  \n",
      " activity in the vacation period will be re -allotted if \n",
      "required. Under no circumstances shall any student occupy \n",
      "a hostel room without prior permission from the wardens \n",
      "and without the knowledge of the caretakers. 28. Change of rooms - Room changes are allowed only after \n",
      "the authorization o f wardens. Room change form must be \n",
      "filled, and approval of warden must be taken be changing \n",
      "rooms. 29. Keeping Pet animals /Taking food out of Mess and \n",
      "feeding pet animal is strictly prohibited. Hostel inmates \n",
      "who are found guilty of misconduct shall be liab le to fine, \n",
      "if the student is found repeating second time a severe \n",
      "punishment including expulsion from the hostel. 30. Senior Batch students are not allowed to enter first year \n",
      "hostels after 09.00 P.M.Students violating rules may be \n",
      "penalized. 31. Unauthorized e\n",
      "**********\n",
      "**********\n",
      "Page: {} Source: N. Reddy, 2nd Edition, Plenum Press.\n",
      "**********\n",
      "**********\n",
      "Page: {} Source: 24. The common area above Mess 2 is accessible to all \n",
      "students from 9AM to 7PM. The Gym above Mess 2 is a \n",
      "girls gym and is out of b ounds for the boys. The boys’ \n",
      "gym is housed in Hostel -1. 25. No student may occupy more than one room at a time. Students who are shifting rooms within hostel, inter hostel  \n",
      "must do so within the timeframe of one day. Any student \n",
      "found to be occupying more than one room at a time will \n",
      "be penalized and additionally o ccupied rooms will be \n",
      "vacated. 26. In the event that a students wants to take leave during the \n",
      "semester he/she must fill out the hostel leave form and get \n",
      "it approved by the warden and hand it over to the caretaker \n",
      "before leave. This must be done in addition to filling out \n",
      "the leave form issued by the DOAA office. Leave will not \n",
      "be approved by wardens unless the academic leave form is \n",
      "first approved. This holds also during the summer and \n",
      "winter breaks when the student is staying on campus as \n",
      "part of any activi ty for which he/she has registered in the \n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "for source in response[\"context\"]:\n",
    "    print(\"*\"*10)\n",
    "    print(f\"Page: {source.metadata} Source: {source.page_content}\")\n",
    "    print(\"*\"*10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b82c20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
